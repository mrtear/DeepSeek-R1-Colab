{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nOiaK78kbX8R",
        "RzqW-K51bvvV"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "  <img src='https://i.ibb.co/RkJM31wR/deepseek-logo.png' height=\"70\" alt=\"DeepSeek Logo\"/>\n",
        "  \n",
        "  [![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/mrtear/DeepSeek-R1-Colab)\n",
        "</div>\n",
        "\n",
        "### üìä Available Models\n",
        "* ‚ö° 1.5b (1.1GB) - Fastest, lightweight model\n",
        "* ‚ö° 7b (4.8GB) - Fast, balanced performance\n",
        "* ‚ö° 8b (4.9GB) - Fast, recommended for most users\n",
        "* üê¢ 14b (9.0GB) - Enhanced capabilities, slower inference\n",
        "* üê¢ 32b (20GB) - Most powerful, requires substantial GPU memory\n",
        "\n",
        "### ‚ö†Ô∏è Important Notes:\n",
        "1. **Runtime Duration (FREE):**\n",
        "   - GPU Runtime: ~1h 20m\n",
        "   - CPU Runtime: ~27h 50m\n",
        "\n",
        "2. **Before Closing:**\n",
        "   - Properly terminate session: `Runtime ‚Üí Disconnect and delete runtime`"
      ],
      "metadata": {
        "id": "7UrWlPZsfiB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src='https://i.ibb.co/ZzGYz0TZ/z9g69f.png' height=\"30\" /></a> <b> <font color=red>Initialize </b>\n",
        "\n",
        "     ‚ñº Run or use the dropdown arrow to view hidden cells. Complete each step before moving to the next."
      ],
      "metadata": {
        "id": "nOiaK78kbX8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0D1mqlQMbOTU"
      },
      "outputs": [],
      "source": [
        "# @title üîß Setting Up Environment\n",
        "\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y pciutils > /dev/null 2>&1\n",
        "!pip install -q ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "\n",
        "print(\"Success! Please proceed to the next step.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üéÆ Check GPU Availability\n",
        "\n",
        "gpu_check = !nvidia-smi --query-gpu=gpu_name --format=csv,noheader 2>/dev/null\n",
        "\n",
        "if gpu_check:\n",
        "    print(f\"GPU {gpu_check[0]} detected! Please proceed to the next step.\")\n",
        "else:\n",
        "    print(\"No GPU found. Please change runtime to GPU in Runtime > Change runtime type.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_VXYiRjCbd_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src='https://i.ibb.co/ZzGYz0TZ/z9g69f.png' height=\"30\" /></a> <b> <font color=cyan>Configure"
      ],
      "metadata": {
        "id": "RzqW-K51bvvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Start Ollama Server\n",
        "# @markdown <b>ü¶ê Run this cell again if you see any connection <font color=red>errors.</b>\n",
        "\n",
        "import subprocess\n",
        "\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "print(\"Success. The server is now running in the background. Proceed to the next step.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FYx8T1Xdb6tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ü§ñ Select and Download DeepSeek R1 Model\n",
        "# @markdown Select your preferred model size. If you change the model, run this cell and the previous one again.\n",
        "\n",
        "model = \"‚ö° 8b (4.9GB)\" # @param [\"‚ö° 1.5b (1.1GB)\", \"‚ö° 7b (4.8GB)\", \"‚ö° 8b (4.9GB)\", \"üê¢ 14b (9.0GB)\", \"üê¢ 32b (20GB)\"]\n",
        "\n",
        "# Extract just the model size without the file size\n",
        "actual_model = model.split()[1].split()[0]\n",
        "\n",
        "try:\n",
        "    !ollama run deepseek-r1:{actual_model} > /dev/null 2>&1\n",
        "    print(\"Success! Please proceed to the next step.\")\n",
        "except Exception as e:\n",
        "    if \"ollama\" in str(e).lower():\n",
        "        print(\"Error: could not connect to ollama app, is it running?\")\n",
        "        print(\"Please run the above 'Ollama Server' cell again.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nqSgeH3Rb7EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìã Available DeepSeek Models\n",
        "# @markdown Shows currently downloaded models and their sizes\n",
        "\n",
        "!ollama list"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z7B5toF-b7nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src='https://i.ibb.co/ZzGYz0TZ/z9g69f.png' height=\"30\" /></a> <b> <font color=orange>Utilize </b>"
      ],
      "metadata": {
        "id": "4CAziIRecagz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <img src='https://i.ibb.co/3yz9bfRR/deepseek-color.png' height=\"30\" /></a> Chat with DeepSeek AI\n",
        "# @markdown Select downloaded model and enter your message below\n",
        "\n",
        "model = \"‚ö° 8b (4.9GB)\" # @param [\"‚ö° 1.5b (1.1GB)\", \"‚ö° 7b (4.8GB)\", \"‚ö° 8b (4.9GB)\", \"üê¢ 14b (9.0GB)\", \"üê¢ 32b (20GB)\"]\n",
        "prompt = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Extract just the model size without the file size\n",
        "actual_model = model.split()[1].split()[0]\n",
        "\n",
        "from ollama import chat\n",
        "from ollama import ChatResponse\n",
        "\n",
        "try:\n",
        "    response: ChatResponse = chat(\n",
        "        model=f\"deepseek-r1:{actual_model}\",\n",
        "        messages=[{\n",
        "            'role': 'user',\n",
        "            'content': prompt,\n",
        "        }]\n",
        "    )\n",
        "    print(response['message']['content'])\n",
        "except Exception as e:\n",
        "    if \"ollama\" in str(e).lower():\n",
        "        print(\"Please run the 'Ollama Server' in configure section again.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f5i2yFa_cfF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}